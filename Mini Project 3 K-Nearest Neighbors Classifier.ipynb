#!/usr/bin/env python
# coding: utf-8

# Introduction
# 
# Briefly introduce the project objectives and the dataset you'll be using (Pima Indians Diabetes Database).
# Outline the steps you'll be taking to achieve the objective.
# Data Exploration and Preprocessing
# 
# Load the dataset into the notebook.
# Perform exploratory data analysis (EDA) to understand the structure and characteristics of the data.
# Check for missing values and handle them appropriately (e.g., imputation).
# Handle any class imbalances if present (e.g., oversampling, undersampling, or using class weights).
# Feature Engineering
# 
# Depending on the dataset, you may want to create new features or transform existing ones to improve model performance.
# Data Splitting
# 
# Split the data into training and testing sets.
# Model Building
# 
# Implement the K-Nearest Neighbors (KNN) classifier using Scikit-learn.
# Train the model on the training data.
# Model Evaluation
# 
# Evaluate the model's performance on the testing data using appropriate evaluation metrics (e.g., accuracy, precision, recall, F1-score).
# Plot a confusion matrix to visualize the model's performance.
# Hyperparameter Tuning
# 
# Use cross-validation to find the optimal number of neighbors (hyperparameter tuning).
# Plot the cross-validation scores for different numbers of neighbors and choose the optimal value.
# Final Model Evaluation
# 
# Evaluate the final model with the optimal hyperparameters on the testing data.
# Compare the performance of the tuned model with the baseline model.
# Conclusion
# 
# Summarize the findings of the analysis.
# Discuss any limitations of the approach and potential areas for improvement.
# Conclude with key takeaways.
# References
# 
# Include any references to datasets, libraries, or papers used in the project.
# Remember to include code cells with appropriate comments to explain each step and Markdown cells for text explanation and documentation. Good luck with your project! If you need further assistance with any specific step, feel free to ask.

# In[ ]:


# Step 1: Introduction (No code needed)

# Step 2: Data Exploration and Preprocessing
import pandas as pd

# Load the dataset
data = pd.read_csv("pima_indians_diabetes.csv")

# Display the first few rows of the dataset
print(data.head())

# Check for missing values
print(data.isnull().sum())

# Handle missing values (for example, by imputing the mean)
data.fillna(data.mean(), inplace=True)

# Handle class imbalances (if any)
# For example, using Synthetic Minority Over-sampling Technique (SMOTE)
from imblearn.over_sampling import SMOTE

X = data.drop('Outcome', axis=1)
y = data['Outcome']

smote = SMOTE()
X_smote, y_smote = smote.fit_resample(X, y)

# Step 3: Feature Engineering (No code provided)

# Step 4: Data Splitting
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.2, random_state=42)

# Step 5: Model Building
from sklearn.neighbors import KNeighborsClassifier

# Initialize the KNN classifier
knn = KNeighborsClassifier()

# Train the model
knn.fit(X_train, y_train)

# Step 6: Model Evaluation
from sklearn.metrics import classification_report, confusion_matrix

# Predict on the testing data
y_pred = knn.predict(X_test)

# Evaluate the model
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

# Step 7: Hyperparameter Tuning
from sklearn.model_selection import GridSearchCV

# Define the parameter grid
param_grid = {'n_neighbors': [3, 5, 7, 9, 11]}

# Initialize GridSearchCV
grid_search = GridSearchCV(knn, param_grid, cv=5)

# Fit GridSearchCV to the data
grid_search.fit(X_train, y_train)

# Print the best hyperparameters
print(grid_search.best_params_)

# Step 8: Final Model Evaluation
# Evaluate the tuned model
tuned_knn = grid_search.best_estimator_
y_tuned_pred = tuned_knn.predict(X_test)

print(classification_report(y_test, y_tuned_pred))
print(confusion_matrix(y_test, y_tuned_pred))

# Step 9: Conclusion (No code needed)


# Make sure to replace "pima_indians_diabetes.csv" with the actual filename of your dataset. Also, ensure you have installed the necessary libraries like pandas, imbalanced-learn, and scikit-learn. This code provides a basic framework for the project; you may need to adjust it based on the specifics of your dataset and analysis requirements.
